"""
ë¡œìŠ¤íŠ¸ì•„í¬ ê²Œì„ í•¸ë“¤ëŸ¬
"""

import logging
from typing import Sequence
from mcp import types

from .base import BaseHandler
from ..scrapers.lost_ark import LostArkScraper
from ..models.game_news import NewsType

logger = logging.getLogger(__name__)


class LostArkHandler(BaseHandler):
    """ë¡œìŠ¤íŠ¸ì•„í¬ ê²Œì„ í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        super().__init__("lost_ark")
        self.scraper = LostArkScraper()
    
    async def get_announcements(self, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
            news_list = await self.scraper.get_announcements()
            
            if not news_list:
                return self._create_success_response("í˜„ì¬ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‰´ìŠ¤ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            result_lines = [f"ğŸ“¢ ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ({len(news_list)}ê°œ)\n"]
            
            for i, news in enumerate(news_list, 1):
                result_lines.append(
                    f"{i}. {news.title}\n"
                    f"   ğŸ”— {news.url}\n"
                    f"   ğŸ“… {news.published_date}\n"
                )
            
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ {len(news_list)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response("\n".join(result_lines))
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ê³µì§€ì‚¬í•­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_announcement_detail(self, url: str, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ: {url}")
            detail = await self.scraper.get_announcement_detail(url)
            
            if not detail:
                return self._create_error_response("í•´ë‹¹ ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            result = f"ğŸ“¢ ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ìƒì„¸\n\n"
            result += f"ì œëª©: {detail.title}\n"
            result += f"URL: {detail.url}\n"
            result += f"ì‘ì„±ì¼: {detail.published_date}\n"
            if detail.summary:
                result += f"ìš”ì•½: {detail.summary}\n"
            if detail.tags:
                result += f"íƒœê·¸: {', '.join(detail.tags)}\n"
            if detail.content:
                result += f"\në‚´ìš©:\n{detail.content}\n"
            
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response(result)
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_events(self, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
            news_list = await self.scraper.get_events()
            
            if not news_list:
                return self._create_success_response("í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‰´ìŠ¤ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            result_lines = [f"ğŸ‰ ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ({len(news_list)}ê°œ)\n"]
            
            for i, news in enumerate(news_list, 1):
                result_lines.append(
                    f"{i}. {news.title}\n"
                    f"   ğŸ”— {news.url}\n"
                    f"   ğŸ“… {news.published_date}\n"
                )
            
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ {len(news_list)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response("\n".join(result_lines))
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì´ë²¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_event_detail(self, url: str, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ: {url}")
            detail = await self.scraper.get_event_detail(url)
            
            if not detail:
                return self._create_error_response("í•´ë‹¹ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            result = f"ğŸ‰ ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ìƒì„¸\n\n"
            result += f"ì œëª©: {detail.title}\n"
            result += f"URL: {detail.url}\n"
            result += f"ì‘ì„±ì¼: {detail.published_date}\n"
            if detail.summary:
                result += f"ìš”ì•½: {detail.summary}\n"
            if detail.tags:
                result += f"íƒœê·¸: {', '.join(detail.tags)}\n"
            if detail.content:
                result += f"\në‚´ìš©:\n{detail.content}\n"
            
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response(result)
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_updates(self, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
            news_list = await self.scraper.get_updates()
            
            if not news_list:
                return self._create_success_response("í˜„ì¬ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‰´ìŠ¤ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            result_lines = [f"ğŸ”„ ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ({len(news_list)}ê°œ)\n"]
            
            for i, news in enumerate(news_list, 1):
                result_lines.append(
                    f"{i}. {news.title}\n"
                    f"   ğŸ”— {news.url}\n"
                    f"   ğŸ“… {news.published_date}\n"
                )
            
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ {len(news_list)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response("\n".join(result_lines))
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì—…ë°ì´íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def get_update_detail(self, url: str, **kwargs) -> Sequence[types.TextContent]:
        """ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            logger.info(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ: {url}")
            detail = await self.scraper.get_update_detail(url)
            
            if not detail:
                return self._create_error_response("í•´ë‹¹ ì—…ë°ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            result = f"ğŸ”„ ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ìƒì„¸\n\n"
            result += f"ì œëª©: {detail.title}\n"
            result += f"URL: {detail.url}\n"
            result += f"ì‘ì„±ì¼: {detail.published_date}\n"
            if detail.summary:
                result += f"ìš”ì•½: {detail.summary}\n"
            if detail.tags:
                result += f"íƒœê·¸: {', '.join(detail.tags)}\n"
            if detail.content:
                result += f"\në‚´ìš©:\n{detail.content}\n"
            
            logger.info("ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì™„ë£Œ")
            return self._create_success_response(result)
            
        except Exception as e:
            logger.error(f"ë¡œìŠ¤íŠ¸ì•„í¬ ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self._create_error_response(f"ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}") 