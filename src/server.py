#!/usr/bin/env python3
"""
ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ MCP ì„œë²„ - ë©”ì¸ ì§„ì…ì 
"""

import asyncio
import logging
import sys
import json
from typing import List, Sequence, Any, Dict

# MCP ê´€ë ¨ import
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.server.models import InitializationOptions
from mcp.types import ServerCapabilities, Tool, TextContent, ToolsCapability

# ê²Œì„ ìŠ¤í¬ë˜í¼ import
from src.scrapers.lordnine import LordnineScraper
from src.scrapers.epic_seven import EpicSevenScraper
from src.scrapers.lost_ark import LostArkScraper
from src.models.exceptions import ScrapingException

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stderr)]
)
logger = logging.getLogger(__name__)

# ì„œë²„ ìƒì„±
app = Server("game-news-scraper")

# ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤
scrapers = {
    "lordnine": LordnineScraper(),
    "epic_seven": EpicSevenScraper(),
    "lost_ark": LostArkScraper()
}

@app.list_tools()
async def list_tools() -> List[Tool]:
    """ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë„êµ¬ ëª©ë¡"""
    logger.info("=== ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë„êµ¬ ëª©ë¡ ìš”ì²­ë¨ ===")
    
    tools = [
        Tool(
            name="get_game_announcements",
            description="ê²Œì„ì˜ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜ (lordnine: ë¡œë“œë‚˜ì¸, epic_seven: ì—í”½ì„¸ë¸, lost_ark: ë¡œìŠ¤íŠ¸ì•„í¬)"
                    },
                    "limit": {
                        "type": "integer",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50,
                        "description": "ì¡°íšŒí•  ê³µì§€ì‚¬í•­ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
                    }
                },
                "required": ["game"]
            }
        ),
        Tool(
            name="get_announcement_detail",
            description="ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜"
                    },
                    "url": {
                        "type": "string",
                        "description": "ê³µì§€ì‚¬í•­ URL"
                    }
                },
                "required": ["game", "url"]
            }
        ),
        Tool(
            name="get_game_events",
            description="ê²Œì„ì˜ ì´ë²¤íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜"
                    },
                    "limit": {
                        "type": "integer",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50,
                        "description": "ì¡°íšŒí•  ì´ë²¤íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
                    }
                },
                "required": ["game"]
            }
        ),
        Tool(
            name="get_event_detail",
            description="ì´ë²¤íŠ¸ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜"
                    },
                    "url": {
                        "type": "string",
                        "description": "ì´ë²¤íŠ¸ URL"
                    }
                },
                "required": ["game", "url"]
            }
        ),
        Tool(
            name="get_game_updates",
            description="ê²Œì„ì˜ ì—…ë°ì´íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜"
                    },
                    "limit": {
                        "type": "integer",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50,
                        "description": "ì¡°íšŒí•  ì—…ë°ì´íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
                    }
                },
                "required": ["game"]
            }
        ),
        Tool(
            name="get_update_detail",
            description="ì—…ë°ì´íŠ¸ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "game": {
                        "type": "string",
                        "enum": ["lordnine", "epic_seven", "lost_ark"],
                        "description": "ê²Œì„ ì¢…ë¥˜"
                    },
                    "url": {
                        "type": "string",
                        "description": "ì—…ë°ì´íŠ¸ URL"
                    }
                },
                "required": ["game", "url"]
            }
        )
    ]
    
    logger.info(f"=== {len(tools)}ê°œ ë„êµ¬ ë°˜í™˜ ===")
    return tools

@app.call_tool()
async def call_tool(name: str, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
    logger.info(f"=== ë„êµ¬ í˜¸ì¶œ: {name}, ì¸ìˆ˜: {arguments} ===")
    
    try:
        game = arguments.get("game")
        if game not in scrapers:
            return [TextContent(type="text", text=f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²Œì„ì…ë‹ˆë‹¤: {game}")]
        
        scraper = scrapers[game]
        
        if name == "get_game_announcements":
            return await handle_get_announcements(scraper, arguments)
        elif name == "get_announcement_detail":
            return await handle_get_announcement_detail(scraper, arguments)
        elif name == "get_game_events":
            return await handle_get_events(scraper, arguments)
        elif name == "get_event_detail":
            return await handle_get_event_detail(scraper, arguments)
        elif name == "get_game_updates":
            return await handle_get_updates(scraper, arguments)
        elif name == "get_update_detail":
            return await handle_get_update_detail(scraper, arguments)
        else:
            return [TextContent(type="text", text=f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}")]
            
    except Exception as e:
        logger.error(f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_announcements(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        limit = arguments.get("limit", 10)
        announcements = await scraper.get_announcements()
        
        if not announcements:
            return [TextContent(type="text", text="ğŸ“‹ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")]
        
        # ì œí•œëœ ê°œìˆ˜ë§Œ ë°˜í™˜
        limited_announcements = announcements[:limit]
        
        result = f"ğŸ“¢ **{scraper.game_type.value} ê³µì§€ì‚¬í•­** ({len(limited_announcements)}ê°œ)\n\n"
        
        for i, news in enumerate(limited_announcements, 1):
            result += f"**{i}. {news.title}**\n"
            result += f"   ğŸ“… {news.published_at.strftime('%Y-%m-%d %H:%M')}\n"
            result += f"   ğŸ”— {news.url}\n"
            if news.tags:
                result += f"   ğŸ·ï¸ {', '.join(news.tags)}\n"
            result += "\n"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ê³µì§€ì‚¬í•­ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ê³µì§€ì‚¬í•­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_announcement_detail(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        url = arguments.get("url")
        if not url:
            return [TextContent(type="text", text="âŒ URLì´ í•„ìš”í•©ë‹ˆë‹¤.")]
        
        detail = await scraper.get_announcement_detail(url)
        
        if not detail:
            return [TextContent(type="text", text="âŒ ê³µì§€ì‚¬í•­ ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]
        
        result = f"ğŸ“¢ **{detail.title}**\n\n"
        result += f"ğŸ“… **ê²Œì‹œì¼:** {detail.published_at.strftime('%Y-%m-%d %H:%M')}\n"
        result += f"ğŸ”— **URL:** {detail.url}\n"
        if detail.tags:
            result += f"ğŸ·ï¸ **íƒœê·¸:** {', '.join(detail.tags)}\n"
        result += "\n"
        
        if detail.content:
            result += "ğŸ“ **ë‚´ìš©:**\n"
            result += detail.content[:1000]  # ë‚´ìš© ì œí•œ
            if len(detail.content) > 1000:
                result += "...\n\n(ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_events(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        limit = arguments.get("limit", 10)
        events = await scraper.get_events()
        
        if not events:
            return [TextContent(type="text", text="ğŸ‰ ì§„í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")]
        
        limited_events = events[:limit]
        
        result = f"ğŸ‰ **{scraper.game_type.value} ì´ë²¤íŠ¸** ({len(limited_events)}ê°œ)\n\n"
        
        for i, news in enumerate(limited_events, 1):
            result += f"**{i}. {news.title}**\n"
            result += f"   ğŸ“… {news.published_at.strftime('%Y-%m-%d %H:%M')}\n"
            result += f"   ğŸ”— {news.url}\n"
            if news.tags:
                result += f"   ğŸ·ï¸ {', '.join(news.tags)}\n"
            result += "\n"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ì´ë²¤íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ì´ë²¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_event_detail(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        url = arguments.get("url")
        if not url:
            return [TextContent(type="text", text="âŒ URLì´ í•„ìš”í•©ë‹ˆë‹¤.")]
        
        detail = await scraper.get_event_detail(url)
        
        if not detail:
            return [TextContent(type="text", text="âŒ ì´ë²¤íŠ¸ ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]
        
        result = f"ğŸ‰ **{detail.title}**\n\n"
        result += f"ğŸ“… **ê²Œì‹œì¼:** {detail.published_at.strftime('%Y-%m-%d %H:%M')}\n"
        result += f"ğŸ”— **URL:** {detail.url}\n"
        if detail.tags:
            result += f"ğŸ·ï¸ **íƒœê·¸:** {', '.join(detail.tags)}\n"
        result += "\n"
        
        if detail.content:
            result += "ğŸ“ **ë‚´ìš©:**\n"
            result += detail.content[:1000]
            if len(detail.content) > 1000:
                result += "...\n\n(ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_updates(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ì—…ë°ì´íŠ¸ ëª©ë¡ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        limit = arguments.get("limit", 10)
        updates = await scraper.get_updates()
        
        if not updates:
            return [TextContent(type="text", text="ğŸ”„ ìµœê·¼ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")]
        
        limited_updates = updates[:limit]
        
        result = f"ğŸ”„ **{scraper.game_type.value} ì—…ë°ì´íŠ¸** ({len(limited_updates)}ê°œ)\n\n"
        
        for i, news in enumerate(limited_updates, 1):
            result += f"**{i}. {news.title}**\n"
            result += f"   ğŸ“… {news.published_at.strftime('%Y-%m-%d %H:%M')}\n"
            result += f"   ğŸ”— {news.url}\n"
            if news.tags:
                result += f"   ğŸ·ï¸ {', '.join(news.tags)}\n"
            result += "\n"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ì—…ë°ì´íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ì—…ë°ì´íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def handle_get_update_detail(scraper, arguments: Dict[str, Any]) -> Sequence[TextContent]:
    """ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì²˜ë¦¬"""
    try:
        url = arguments.get("url")
        if not url:
            return [TextContent(type="text", text="âŒ URLì´ í•„ìš”í•©ë‹ˆë‹¤.")]
        
        detail = await scraper.get_update_detail(url)
        
        if not detail:
            return [TextContent(type="text", text="âŒ ì—…ë°ì´íŠ¸ ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]
        
        result = f"ğŸ”„ **{detail.title}**\n\n"
        result += f"ğŸ“… **ê²Œì‹œì¼:** {detail.published_at.strftime('%Y-%m-%d %H:%M')}\n"
        result += f"ğŸ”— **URL:** {detail.url}\n"
        if detail.tags:
            result += f"ğŸ·ï¸ **íƒœê·¸:** {', '.join(detail.tags)}\n"
        result += "\n"
        
        if detail.content:
            result += "ğŸ“ **ë‚´ìš©:**\n"
            result += detail.content[:1000]
            if len(detail.content) > 1000:
                result += "...\n\n(ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        
        return [TextContent(type="text", text=result)]
        
    except Exception as e:
        logger.error(f"ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}", exc_info=True)
        return [TextContent(type="text", text=f"âŒ ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]

async def main():
    logger.info("=== ê²Œì„ ë‰´ìŠ¤ ìˆ˜ì§‘ MCP ì„œë²„ ì‹œì‘ ===")
    
    try:
        async with stdio_server() as (read_stream, write_stream):
            logger.info("=== STDIO ì„œë²„ ì‹œì‘ë¨ ===")
            
            # ëª…ì‹œì ìœ¼ë¡œ íˆ´ ê¸°ëŠ¥ í™œì„±í™”
            capabilities = ServerCapabilities(
                tools=ToolsCapability(listChanged=True)
            )
            
            await app.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="game-news-scraper",
                    server_version="1.0.0",
                    capabilities=capabilities
                )
            )
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
        raise

if __name__ == "__main__":
    asyncio.run(main()) 