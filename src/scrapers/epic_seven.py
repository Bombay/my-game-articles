"""ì—í”½ì„¸ë¸ ê²Œì„ ìŠ¤í¬ë˜í¼"""

import re
from typing import List, Optional, Dict, Any
from datetime import datetime

from src.scrapers.base import BaseScraper
from src.models.game_news import GameNews, GameType, NewsType
from src.models.exceptions import ScrapingException, ApiException
from src.utils.helpers import parse_timestamp, clean_text


class EpicSevenScraper(BaseScraper):
    """ì—í”½ì„¸ë¸ ê²Œì„ ìŠ¤í¬ë˜í¼"""
    
    # API ì„¤ì •
    BASE_URL = "https://api.onstove.com/cwms/v3.0"
    
    # ê²Œì‹œíŒ ID (board_seq)
    BOARD_SEQ = {
        "announcements": "995",   # ê³µì§€ì‚¬í•­
        "events": "1000",         # ì´ë²¤íŠ¸
        "updates": "997"          # ì—…ë°ì´íŠ¸
    }
    
    # ê³µí†µ API íŒŒë¼ë¯¸í„°
    COMMON_PARAMS = {
        "interaction_type_code": "LIKE,DISLIKE,COMMENT,VIEW",
        "content_yn": "Y",
        "summary_yn": "Y",
        "sort_type_code": "LATEST",
        "headline_title_yn": "Y",
        "translation_yn": "N",
        "page": 1,
        "size": 20  # ì—í”½ì„¸ë¸ì€ 20ê°œ ê³ ì •
    }
    
    def __init__(self, timeout: int = 30):
        """ì—í”½ì„¸ë¸ ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        super().__init__(GameType.EPIC_SEVEN, timeout)
        
    def get_default_headers(self) -> dict:
        """ì—í”½ì„¸ë¸ APIìš© ê¸°ë³¸ í—¤ë”"""
        headers = super().get_default_headers()
        headers.update({
            'Accept': 'application/json',
            'Referer': 'https://page.onstove.com/epicseven/global',
            'Origin': 'https://page.onstove.com',
            'x-client-lang': 'ko'
        })
        return headers
    
    async def get_announcements(self) -> List[GameNews]:
        """ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ"""
        try:
            url = f"{self.BASE_URL}/article_group/BOARD/{self.BOARD_SEQ['announcements']}/article/list"
            response = await self.make_request(url, params=self.COMMON_PARAMS)
            data = response.json()
            
            if not self.validate_response_data(data, ['value']):
                raise ScrapingException("ê³µì§€ì‚¬í•­ ì‘ë‹µ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            value_data = data.get('value', {})
            if 'list' not in value_data:
                raise ScrapingException("ê³µì§€ì‚¬í•­ ì‘ë‹µì— 'list' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            articles = value_data.get('list', [])
            news_list = []
            
            for article in articles:
                try:
                    news = self._parse_article_data(article, NewsType.ANNOUNCEMENT)
                    if news:
                        news_list.append(news)
                except Exception as e:
                    # ê°œë³„ í•­ëª© íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
                    continue
            
            return news_list
            
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"ê³µì§€ì‚¬í•­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def get_announcement_detail(self, url: str) -> Optional[GameNews]:
        """ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ"""
        return await self._get_detail(url, NewsType.ANNOUNCEMENT)
    
    async def get_events(self) -> List[GameNews]:
        """ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            url = f"{self.BASE_URL}/article_group/BOARD/{self.BOARD_SEQ['events']}/article/list"
            response = await self.make_request(url, params=self.COMMON_PARAMS)
            data = response.json()
            
            if not self.validate_response_data(data, ['value']):
                raise ScrapingException("ì´ë²¤íŠ¸ ì‘ë‹µ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            value_data = data.get('value', {})
            if 'list' not in value_data:
                raise ScrapingException("ì´ë²¤íŠ¸ ì‘ë‹µì— 'list' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            articles = value_data.get('list', [])
            news_list = []
            
            for article in articles:
                try:
                    news = self._parse_article_data(article, NewsType.EVENT)
                    if news:
                        news_list.append(news)
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"ì´ë²¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def get_event_detail(self, url: str) -> Optional[GameNews]:
        """ì´ë²¤íŠ¸ ìƒì„¸ ì¡°íšŒ"""
        return await self._get_detail(url, NewsType.EVENT)
    
    async def get_updates(self) -> List[GameNews]:
        """ì—…ë°ì´íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            url = f"{self.BASE_URL}/article_group/BOARD/{self.BOARD_SEQ['updates']}/article/list"
            response = await self.make_request(url, params=self.COMMON_PARAMS)
            data = response.json()
            
            if not self.validate_response_data(data, ['value']):
                raise ScrapingException("ì—…ë°ì´íŠ¸ ì‘ë‹µ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            value_data = data.get('value', {})
            if 'list' not in value_data:
                raise ScrapingException("ì—…ë°ì´íŠ¸ ì‘ë‹µì— 'list' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            articles = value_data.get('list', [])
            news_list = []
            
            for article in articles:
                try:
                    news = self._parse_article_data(article, NewsType.UPDATE)
                    if news:
                        news_list.append(news)
                except Exception as e:
                    continue
            
            return news_list
            
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"ì—…ë°ì´íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def get_update_detail(self, url: str) -> Optional[GameNews]:
        """ì—…ë°ì´íŠ¸ ìƒì„¸ ì¡°íšŒ"""
        return await self._get_detail(url, NewsType.UPDATE)
    
    async def _get_detail(self, url: str, category: NewsType) -> Optional[GameNews]:
        """ìƒì„¸ ì •ë³´ ì¡°íšŒ ê³µí†µ ë©”ì„œë“œ"""
        try:
            article_id = self._extract_article_id_from_url(url)
            if not article_id:
                raise ScrapingException(f"URLì—ì„œ article_idë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
            
            # ìƒì„¸ ì •ë³´ API êµ¬í˜„
            detail_url = f"{self.BASE_URL}/article/{article_id}"
            params = {
                "interaction_type_code": "LIKE,DISLIKE,COMMENT,VIEW",
                "content_yn": "Y"
            }
            
            try:
                response = await self.make_request(detail_url, params=params)
                data = response.json()
                
                if not self.validate_response_data(data, ['value']):
                    raise ScrapingException("ìƒì„¸ ì •ë³´ ì‘ë‹µ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                
                article = data.get('value', {})
                return self._parse_article_detail(article, category)
                
            except Exception as e:
                # ìƒì„¸ API ì‹¤íŒ¨ ì‹œ ëª©ë¡ì—ì„œ ì°¾ê¸° (fallback)
                return await self._get_detail_from_list(article_id, category)
            
        except Exception as e:
            if isinstance(e, ScrapingException):
                raise
            raise ScrapingException(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    async def _get_detail_from_list(self, article_id: str, category: NewsType) -> Optional[GameNews]:
        """ëª©ë¡ì—ì„œ ìƒì„¸ ì •ë³´ ì°¾ê¸° (fallback)"""
        try:
            # ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ì ì ˆí•œ ëª©ë¡ ì¡°íšŒ
            if category == NewsType.ANNOUNCEMENT:
                articles_list = await self.get_announcements()
            elif category == NewsType.EVENT:
                articles_list = await self.get_events()
            elif category == NewsType.UPDATE:
                articles_list = await self.get_updates()
            else:
                articles_list = await self.get_announcements()
            
            # article_idê°€ ì¼ì¹˜í•˜ëŠ” í•­ëª© ì°¾ê¸°
            for article in articles_list:
                if article.id == article_id:
                    # summaryë¥¼ contentë¡œ ë³µì‚¬í•˜ì—¬ ìƒì„¸ ì •ë³´ì²˜ëŸ¼ ë§Œë“¤ê¸°
                    article.content = article.summary
                    return article
            
            return None
            
        except Exception as e:
            raise ScrapingException(f"ëª©ë¡ì—ì„œ ìƒì„¸ ì •ë³´ ì°¾ê¸° ì‹¤íŒ¨: {str(e)}")
    
    def _parse_article_data(self, article: Dict[str, Any], category: NewsType) -> Optional[GameNews]:
        """ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ GameNewsë¡œ ë³€í™˜"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not all(key in article for key in ['article_id', 'title', 'create_datetime']):
                return None
            
            article_id = str(article['article_id'])
            title = article['title']
            create_datetime = article['create_datetime']
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
            if isinstance(create_datetime, (int, float)):
                published_at = datetime.fromtimestamp(create_datetime / 1000)
            else:
                published_at = parse_timestamp(create_datetime)
            
            # URL ìƒì„±
            url = f"https://page.onstove.com/epicseven/global/view/{article_id}"
            
            # ë‚´ìš© ë° ìš”ì•½
            content = article.get('content', '')
            summary = article.get('summary', '')
            
            # ì¡°íšŒìˆ˜
            view_count = article.get('view_count', 0)
            
            # ì¤‘ìš”ë„ íŒë‹¨
            is_important = self._is_important_article(article)
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = self._extract_tags(article)
            
            # ì œëª© ì¥ì‹
            decorated_title = self._decorate_title(title, category, is_important, view_count)
            
            return self.create_game_news(
                id=article_id,
                title=decorated_title,
                url=url,
                published_at=published_at,
                category=category,
                content=content,
                summary=summary,
                is_important=is_important,
                tags=tags,
                view_count=view_count
            )
            
        except Exception as e:
            return None
    
    def _parse_article_detail(self, article: Dict[str, Any], category: NewsType) -> Optional[GameNews]:
        """ìƒì„¸ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ GameNewsë¡œ ë³€í™˜"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not all(key in article for key in ['article_id', 'title', 'create_datetime']):
                return None
            
            article_id = str(article['article_id'])
            title = article['title']
            create_datetime = article['create_datetime']
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
            if isinstance(create_datetime, (int, float)):
                published_at = datetime.fromtimestamp(create_datetime / 1000)
            else:
                published_at = parse_timestamp(create_datetime)
            
            # URL ìƒì„±
            url = f"https://page.onstove.com/epicseven/global/view/{article_id}"
            
            # ë‚´ìš© ë° ìš”ì•½ (ìƒì„¸ ì •ë³´ì—ì„œëŠ” contentê°€ ë” ìƒì„¸í•¨)
            content = article.get('content', '')
            summary = article.get('summary', '')
            
            # ì¡°íšŒìˆ˜
            view_count = article.get('view_count', 0)
            
            # ì¤‘ìš”ë„ íŒë‹¨
            is_important = self._is_important_article(article)
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = self._extract_tags(article)
            
            # ì œëª© ì¥ì‹
            decorated_title = self._decorate_title(title, category, is_important, view_count)
            
            return self.create_game_news(
                id=article_id,
                title=decorated_title,
                url=url,
                published_at=published_at,
                category=category,
                content=content,
                summary=summary,
                is_important=is_important,
                tags=tags,
                view_count=view_count
            )
            
        except Exception as e:
            return None
    
    def _extract_article_id_from_url(self, url: str) -> Optional[str]:
        """URLì—ì„œ article_id ì¶”ì¶œ"""
        if not isinstance(url, str):
            return None
        
        # ì—í”½ì„¸ë¸ URL íŒ¨í„´: https://page.onstove.com/epicseven/global/view/10867009
        match = re.search(r'/view/(\d+)', url)
        if match:
            return match.group(1)
        
        # ë‹¤ë¥¸ íŒ¨í„´ë“¤ë„ ì‹œë„
        patterns = [
            r'article_id=(\d+)',      # article_id=12345
            r'/(\d+)$',               # ëì— ìˆëŠ” ìˆ«ì
            r'/(\d+)/',               # ì¤‘ê°„ì— ìˆëŠ” ìˆ«ì
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def _is_important_article(self, article: Dict[str, Any]) -> bool:
        """ê²Œì‹œê¸€ ì¤‘ìš”ë„ íŒë‹¨"""
        title = article.get('title', '').lower()
        
        # ì¤‘ìš” í‚¤ì›Œë“œ
        important_keywords = [
            'ê¸´ê¸‰', 'ì¤‘ìš”', 'ì ê²€', 'ê³µì§€', 'ì•ˆë‚´', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜',
            'ì´ë²¤íŠ¸', 'ì¶œì‹œ', 'ë¦´ë¦¬ìŠ¤', 'ì˜¤í”ˆ', 'ì¢…ë£Œ', 'ë§ˆê°'
        ]
        
        # ì œëª©ì— ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if any(keyword in title for keyword in important_keywords):
            return True
        
        # ê³ ì • ê²Œì‹œë¬¼ ì—¬ë¶€ í™•ì¸
        if article.get('is_headline', False) or article.get('is_top', False):
            return True
        
        # ì¡°íšŒìˆ˜ê°€ ë†’ì€ ê²½ìš°
        view_count = article.get('view_count', 0)
        if view_count > 10000:
            return True
        
        return False
    
    def _extract_tags(self, article: Dict[str, Any]) -> List[str]:
        """ê²Œì‹œê¸€ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        
        title = article.get('title', '')
        
        # ë§ë¨¸ë¦¬ ì¶”ì¶œ (ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)
        bracket_matches = re.findall(r'\[([^\]]+)\]', title)
        tags.extend(bracket_matches)
        
        # ì¹´í…Œê³ ë¦¬ ì •ë³´
        category = article.get('category', '')
        if category:
            tags.append(category)
        
        # ê³µì‹ íƒ€ì…
        article_type = article.get('article_type', '')
        if article_type:
            tags.append(article_type)
        
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        title_keywords = ['ì ê²€', 'ì—…ë°ì´íŠ¸', 'íŒ¨ì¹˜', 'ì´ë²¤íŠ¸', 'ì¶œì‹œ', 'ì¢…ë£Œ']
        for keyword in title_keywords:
            if keyword in title:
                tags.append(keyword)
        
        return list(set(tags))  # ì¤‘ë³µ ì œê±°
    
    def _decorate_title(self, title: str, category: NewsType, is_important: bool, view_count: int) -> str:
        """ì œëª© ì¥ì‹ (ì´ëª¨ì§€ ì¶”ê°€)"""
        decorated_title = title
        
        # ì¤‘ìš”ë„ ì´ëª¨ì§€
        if is_important:
            decorated_title = f"ğŸ“Œ {decorated_title}"
        
        # ì¡°íšŒìˆ˜ ì´ëª¨ì§€ (10,000 ì´ìƒ)
        if view_count >= 10000:
            decorated_title = f"ğŸ”¥ {decorated_title}"
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì´ëª¨ì§€
        if category == NewsType.ANNOUNCEMENT:
            decorated_title = f"ğŸ“¢ {decorated_title}"
        elif category == NewsType.EVENT:
            decorated_title = f"ğŸ‰ {decorated_title}"
        elif category == NewsType.UPDATE:
            decorated_title = f"ğŸ”„ {decorated_title}"
        
        return decorated_title 